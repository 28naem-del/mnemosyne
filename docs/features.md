# Features Deep Dive

## Overview

Mnemosyne implements a **5-layer cognitive architecture** comprising **33 distinct features** that transform raw text into structured, self-improving, interconnected memory. Each layer builds on the one below it, creating a stack that progresses from raw infrastructure up to autonomous self-improvement -- mirroring how biological memory systems organize themselves from neural substrate to metacognition.

The five layers are:

- **Layer 1 -- Infrastructure (6 features):** The physical foundation. Vector storage, caching, pub/sub communication, knowledge graph storage, the bi-temporal data model, and soft-delete architecture.
- **Layer 2 -- Pipeline (6 features):** The ingestion engine. A deterministic 12-step pipeline, the zero-LLM design philosophy, security filtering, smart deduplication and merge, entity extraction, and the 7-type memory taxonomy.
- **Layer 3 -- Knowledge Graph (5 features):** The connective tissue. Temporal queries, auto-linking, path finding, timeline reconstruction, and depth-limited traversal.
- **Layer 4 -- Cognitive (6 features):** The intelligence layer. Activation decay, multi-signal scoring, intent-aware retrieval, diversity reranking, the 4-tier confidence system, and priority scoring.
- **Layer 5 -- Self-Improvement (10 features):** The autonomic layer. Reinforcement learning, active consolidation, flash reasoning, agent awareness (ToMA), cross-agent synthesis, proactive recall, session survival, observational memory, procedural memory / skill library, and mesh sync.

Every feature described in this document is production-deployed and running at scale across a 10-agent mesh managing 13,000+ memories with sub-200ms retrieval latency. Nothing here is a roadmap item or proof of concept.

---

## Layer 1: Infrastructure (6 Features)

The infrastructure layer provides the physical substrate for all memory operations. It manages persistent storage, high-speed caching, real-time inter-agent communication, graph storage, and the temporal data model that underpins every memory cell. These components are designed to be independently scalable and operationally resilient -- if an optional service goes down, the system degrades gracefully rather than failing.

---

### Feature 1: Vector Storage

**What it does:** Mnemosyne stores all memory embeddings in Qdrant, a purpose-built vector database using HNSW (Hierarchical Navigable Small World) indexing for sub-linear search performance. Every memory is represented as a 768-dimensional vector generated by a Nomic-architecture embedding model, paired with a rich metadata payload of 23 fields.

**How it works:** The system maintains 4 isolated collections -- shared memories, private memories, agent profiles, and skill library -- each serving a distinct purpose within the cognitive architecture. Qdrant's HNSW index builds a multi-layered graph of the vector space where each layer contains a progressively sparser subset of vectors. During search, traversal begins at the sparsest layer and descends through progressively denser layers, achieving logarithmic search complexity instead of linear. The 768-dimensional embedding space captures fine-grained semantic distinctions: the distance between "Redis runs on port 6379" and "Redis server is configured on port 6379" is near-zero (approximately 0.95 cosine similarity), while the distance to "PostgreSQL uses port 5432" is moderate (approximately 0.72), and the distance to "The weather is nice today" is high (approximately 0.15).

**Why it matters:** HNSW indexing ensures that search performance remains sub-linear even as collections grow into millions of vectors. Without it, every recall query would require a full scan of all stored embeddings -- O(n) complexity that becomes unusable at scale. With HNSW, the same query completes in O(log n) time, delivering consistent sub-200ms latency regardless of collection size. The 4-collection isolation prevents cross-contamination: a private session snapshot never surfaces during a fleet-wide knowledge query, and procedural skills do not compete for relevance with transient episodic memories.

**Code example:**

```typescript
// Vector search happens transparently during recall
const results = await m.recall("deployment procedures for production");
// Under the hood: embed query -> HNSW search across relevant collections
// -> return top-k vectors with full 23-field metadata payloads

// Direct collection targeting for specialized queries
const skills = await m.recall("how to restart the auth service", {
  collection: "skills" // Search only the skill library
});

// The embedding service is abstracted behind OpenAI-compatible interface
// Swap between local (MLX, Ollama) or cloud (OpenAI, Cohere) with zero code changes
// Embeddings are LRU-cached (512 entries, ~15ms fresh, ~0ms cached)
```

---

### Feature 2: 2-Tier Cache

**What it does:** Mnemosyne implements a two-tier caching strategy that delivers sub-10ms cached recall with a combined hit rate exceeding 60% in typical conversational workloads. The first tier is process-local; the second is distributed via Redis.

**How it works:** L1 is an in-memory LRU cache holding up to 50 entries with a 5-minute TTL. It operates at near-zero latency (microseconds) since it requires no network round-trip. It captures the temporal locality inherent in agent conversations -- when an agent asks about "deployment" and then immediately follows up with "tell me more about deployment steps", the second query hits L1 directly. L2 is a Redis-backed distributed cache with a 1-hour TTL and pattern-based invalidation. When a memory is stored, updated, or deleted, the broadcast system fires a cache invalidation event that clears relevant L2 entries across all connected Mnemosyne instances. Cache keys are derived from a hash of the query embedding, ensuring that semantically identical queries hit the same cache entry.

**Why it matters:** Recall latency is critical for agent responsiveness. Without caching, every recall requires a full vector search (approximately 150ms). With L1 caching, repeat queries within the same conversation are served in under 0.1ms. With L2 caching, cross-agent repeat queries are served in under 2ms. The invalidation system ensures cache coherence: you never get stale results after a memory store, update, or delete.

**Code example:**

```typescript
// Cache behavior is fully transparent to the caller
const memories = await m.recall("user preferences");
// First call: L1 miss -> L2 miss -> full vector search (~150ms) -> populate both caches
// Second call within 5 min: L1 hit (~0.1ms)
// Call from different agent within 1 hour: L2 hit (~2ms)
// After memory_store invalidation: both caches cleared, next call goes to vector

// Cache statistics (operational monitoring)
// L1: 50 entries max, 5-min TTL, LRU eviction
// L2: 1-hour TTL, pattern-based invalidation via broadcast
// Combined hit rate in production: >60% for conversational workloads
```

---

### Feature 3: Pub/Sub Broadcast

**What it does:** In a multi-agent mesh, every agent needs to know when the shared knowledge base changes. Mnemosyne uses Redis Pub/Sub to propagate typed memory events in real time across all connected instances, enabling immediate cache invalidation, conflict awareness, and priority routing.

**How it works:** Events are structured with a type (e.g., `memory:stored`, `memory:conflict`, `memory:forgotten`), a priority level (`critical` or `normal`), and a payload containing the memory's ID, type, creating agent, extracted entities, and a summary sufficient for other agents to make routing decisions without querying the database. When a critical memory is stored -- say, a production incident report -- it is broadcast with `critical` priority, which triggers immediate cache invalidation and optional priority notification handling in subscribing agents. Lower-priority events like routine memory storage are broadcast at `normal` priority. Subscription is channel-based: agents can subscribe to specific event types or receive all events.

**Why it matters:** Without real-time broadcast, multi-agent memory would be eventually consistent at best -- an agent might serve stale cached results for up to an hour after another agent stores a critical update. With broadcast, every agent in the mesh has a current view of the shared knowledge base within milliseconds of any change. The broadcast system is also foundational to several higher-layer features: cache invalidation depends on it for coherence, cross-agent synthesis depends on it for corroboration awareness, and the Agent Awareness Engine depends on it for tracking what each agent knows.

**Code example:**

```typescript
// Broadcast happens automatically during store/forget/consolidate
await m.store("CRITICAL: Production database is down on example-db", {
  importance: 1.0
});
// This triggers:
// 1. memory:stored event on the broadcast channel
// 2. Cache invalidation event (clears related L2 entries on all agents)
// 3. Priority routing because urgency classifier flags "CRITICAL"

// Agents can subscribe to specific event types
m.on('memory:conflict', (event) => {
  console.log(`Conflict detected: ${event.existingId} vs ${event.newId}`);
});

m.on('memory:stored', (event) => {
  if (event.priority === 'critical') {
    // Handle critical memory immediately
    console.log(`Critical memory from ${event.agentId}: ${event.summary}`);
  }
});
```

---

### Feature 4: Knowledge Graph

**What it does:** Mnemosyne maintains a temporal knowledge graph on FalkorDB (RedisGraph-compatible) that captures entity relationships and enables multi-hop reasoning. While vector search finds memories by semantic similarity, the knowledge graph finds memories by structural relationships -- who is connected to what, what changed when, and how entities relate to each other across time.

**How it works:** The graph uses a deliberately minimal schema with three relationship types: `Memory --MENTIONS--> Entity`, `Memory --CREATED_BY--> Agent`, and `Entity --RELATES_TO--> Entity`. Each relationship carries a `since` timestamp from the memory's `eventTime`, enabling temporal graph queries. When a new memory mentions two entities that already exist in the graph, a `RELATES_TO` edge is created (or updated) between them, strengthening the connection and updating the temporal marker. The graph is continuously updated as new memories are ingested -- it is a living model of the system's knowledge, not a one-time extraction. FalkorDB executes Cypher queries against an in-memory adjacency-list representation, delivering sub-millisecond traversal performance for typical query depths.

**Why it matters:** Vector search answers "what is semantically similar to X?" but cannot answer "how is entity A related to entity B through intermediary entities?" The knowledge graph fills this gap, enabling path finding, timeline reconstruction, and multi-hop reasoning that are impossible with flat vector search. It is the structural backbone that Flash Reasoning (Layer 5) traverses to build multi-step reasoning chains.

**Code example:**

```typescript
// Graph grows automatically as memories are stored
await m.store("Alice deployed the new auth service to my-host on February 20th");
// Graph ingestion creates:
// (Memory) --MENTIONS--> (Alice)
// (Memory) --MENTIONS--> (auth service)
// (Memory) --MENTIONS--> (my-host)
// (Memory) --MENTIONS--> (February 20th)
// (Alice) --RELATES_TO--> (auth service)     [since: 2026-02-20]
// (Alice) --RELATES_TO--> (my-host)         [since: 2026-02-20]
// (auth service) --RELATES_TO--> (my-host)  [since: 2026-02-20]

// Graph schema:
// (Memory) --MENTIONS--> (Entity)
// (Memory) --CREATED_BY--> (Agent)
// (Entity) --RELATES_TO--> (Entity)
```

---

### Feature 5: Bi-Temporal Data Model

**What it does:** Every memory in Mnemosyne carries two independent timestamps: `eventTime` (when the event or fact actually occurred in the real world) and `ingestedAt` (when the memory was stored in the system). This bi-temporal model enables a class of queries that single-timestamp systems simply cannot answer.

**How it works:** The `eventTime` is either explicitly provided by the caller or inferred from the memory text (using date extraction in the pipeline). The `ingestedAt` is automatically set to the current time when the memory is stored. Both timestamps are indexed in Qdrant as payload fields, enabling filtered queries on either temporal dimension. A third timestamp, `updatedAt`, tracks the most recent modification to the memory (metadata updates, merge operations, etc.). The bi-temporal model propagates to the knowledge graph layer, where relationship edges carry the `eventTime` as their `since` timestamp, and to the cognitive layer, where the activation decay model uses `accessTimes` (an array of timestamps) to compute decay curves.

**Why it matters:** Consider the difference: "What did we know about the production outage as of Monday morning?" requires filtering on `ingestedAt` -- it asks what was in the system at that time. "What happened to production last Friday?" requires filtering on `eventTime` -- it asks about the real-world timeline. Without bi-temporal tracking, these questions are indistinguishable. The model also powers timeline reconstruction, temporal graph queries, and the recency signal in multi-signal scoring.

**Code example:**

```typescript
// Store a memory with an explicit event time
await m.store("Database migration completed successfully", {
  eventTime: "2026-02-20T14:30:00Z" // When it actually happened
});
// ingestedAt is automatically set to now (2026-02-23T10:00:00Z)

// Query by event time: "what happened on Friday?"
const friday_events = await m.recall("what happened on Friday", {
  eventTimeAfter: "2026-02-20T00:00:00Z",
  eventTimeBefore: "2026-02-21T00:00:00Z"
});

// Query by ingestion time: "what did we learn today?"
const todays_knowledge = await m.recall("recent updates", {
  ingestedAfter: "2026-02-23T00:00:00Z"
});

// Both dimensions coexist: a memory about a January event
// ingested in February is findable by either timeline
```

---

### Feature 6: Soft-Delete Architecture

**What it does:** Mnemosyne never physically deletes a memory. When `memory_forget` is called, the memory's `deleted` flag is set to `true`, and it is excluded from all future searches. The actual data remains in storage, providing a complete audit trail and enabling recovery when needed.

**How it works:** Soft-deleted memories are automatically excluded from all search operations via Qdrant's payload filtering -- the HNSW index naturally skips filtered-out points with near-zero performance overhead. The consolidation system can still examine deleted memories when detecting contradictions: a memory that was deleted because it was wrong still carries information about what was once believed to be true. The system's historical state can always be reconstructed for debugging or compliance purposes. If storage reclamation is ever needed, a separate maintenance process can permanently purge memories that have been soft-deleted beyond a configurable retention period, but this is always an explicit operational decision.

**Why it matters:** Accidental deletions are always recoverable. The audit trail is complete. Deleted memories still inform the consolidation engine about what was once believed to be true. And the system's historical state can be reconstructed at any point in time. The performance cost is negligible because Qdrant's filtering operates at the index level, not as a post-processing step.

**Code example:**

```typescript
// Forget by semantic search
await m.forget({ query: "old deployment procedure" });
// Memory's deleted flag set to true
// Excluded from all future searches
// Still exists in storage for audit/recovery

// Forget by ID (supports short IDs)
await m.forget({ memoryId: "a1b2c3d4" });
// Short ID resolves to full UUID
// Broadcasts invalidation event to agent mesh

// Recovery is possible via direct Qdrant query with deleted:true filter
// No data is ever permanently lost unless explicitly purged
```

---

## Layer 2: Pipeline (6 Features)

The pipeline layer is the heart of Mnemosyne's ingestion system. It transforms raw text into fully classified, scored, linked, and indexed memory cells through a deterministic 12-step process that completes in under 50 milliseconds with zero LLM calls.

---

### Feature 7: 12-Step Ingestion Pipeline

**What it does:** Every memory passes through 12 sequential processing steps that transform raw text into a fully classified, scored, linked, and indexed memory cell with 23 metadata fields. The pipeline is deterministic: the same input always produces the same output.

**How it works:** The 12 steps execute in strict order:

| Step | Component | What It Does |
|------|-----------|-------------|
| 1 | **Security Filter** | 3-tier classification (public/private/secret). Blocks secrets before embedding. |
| 2 | **Embedding Generation** | Converts text to 768-dimensional vector with LRU caching (512 entries). |
| 3 | **Deduplication & Merge** | Cosine similarity check: >=0.92 = duplicate, 0.70-0.92 = conflict zone. |
| 4 | **Extraction Pipeline** | Extracts entities, memory type, and domain. Zero LLM calls. |
| 5 | **Urgency Classification** | 4-level urgency: critical, important, reference, background. |
| 6 | **Domain Classification** | 5 domains: technical, personal, project, knowledge, general. |
| 7 | **Priority Scoring** | 0.0-1.0 composite score from urgency x domain matrix. |
| 8 | **Confidence Rating** | Multi-signal confidence from retrieval quality, cross-agent agreement, source trust. |
| 9 | **Vector Storage** | Written to appropriate collection with full 23-field metadata payload. |
| 10 | **Auto-Linking** | Bidirectional links to related memories (similarity > 0.70). |
| 11 | **Graph Ingestion** | Entities and relationships added to temporal knowledge graph. |
| 12 | **Broadcast** | Published to mesh via typed channels. Critical memories get priority routing. |

Each step is independently testable, and failures at optional steps (e.g., graph ingestion when FalkorDB is unavailable) do not block the pipeline -- the system gracefully skips and continues.

**Why it matters:** The 12-step pipeline ensures that every memory enters the system fully enriched with classification, scoring, linking, and indexing metadata. This upfront investment during ingestion pays dividends during retrieval: the cognitive layer has all the signals it needs (type, urgency, domain, priority, confidence, linked memories, entities) pre-computed and ready for multi-signal scoring. Without this pipeline, retrieval would require expensive on-the-fly computation of these signals.

**Code example:**

```typescript
const result = await m.store("CRITICAL: Auth service JWT expiry changed from 1hr to 30min");
// Step 1:  Security filter -> public (no secrets detected)
// Step 2:  Embedding -> 768-dim vector generated (~15ms)
// Step 3:  Dedup check -> no duplicates found (novel memory)
// Step 4:  Extraction -> entities: ["Auth service", "JWT", "1hr", "30min"]
//                        type: semantic, domain: technical
// Step 5:  Urgency -> critical ("CRITICAL" keyword)
// Step 6:  Domain -> technical (auth, JWT, service keywords)
// Step 7:  Priority -> 1.0 (critical + technical = maximum)
// Step 8:  Confidence -> 0.70 (Grounded, single-source)
// Step 9:  Vector storage -> written to shared memories collection
// Step 10: Auto-link -> linked to 2 existing JWT-related memories
// Step 11: Graph -> Auth service --RELATES_TO--> JWT edges created
// Step 12: Broadcast -> critical priority event published to mesh

// result: { status: "created", linkedCount: 2 }
// Total time: < 50ms
```

---

### Feature 8: Zero-LLM Pipeline

**What it does:** The entire ingestion pipeline -- classification, entity extraction, urgency detection, domain classification, conflict resolution, and type detection -- runs without any LLM calls. Every step uses algorithmic methods: regular expressions, keyword matching, pattern detection, and heuristic rules.

**How it works:** Memory type detection uses pattern matching for procedural language ("step 1", "how to", "run the command"), preferences ("prefer", "always use", "like"), relationships ("works with", "reports to"), and profiles (structured capability descriptions). Urgency detection scans for keyword families: critical ("CRITICAL", "urgent", "emergency", "down", "broken"), important ("deploy", "release", "deadline", "blocked"), reference (technical facts, specifications), and background (casual observations). Entity extraction uses context-aware regular expressions -- an IP address is only extracted if it appears near network-context words ("server", "connect", "host"), not in version numbers or mathematical expressions. Domain classification uses keyword families mapped to the 5 domains.

**Why it matters:** The zero-LLM design delivers four critical advantages:
1. **Deterministic behavior:** Same input always produces the same classification. No stochastic variance, no prompt sensitivity, no model version drift.
2. **Sub-50ms latency:** No waiting for model inference during storage. The entire pipeline completes faster than a single LLM API call would take just to process the request.
3. **Zero additional cost:** No per-memory API charges. Storing 13,000 memories costs nothing beyond the embedding generation.
4. **Offline capability:** Works without internet after the initial embedding step. The pipeline can run entirely on local infrastructure.

**Code example:**

```typescript
// All classification is algorithmic -- no LLM calls anywhere
await m.store("To deploy to production: 1) Run tests 2) Build Docker image 3) Push to registry 4) Kubectl apply");
// Type detection: "1) ... 2) ... 3) ... 4)" sequential pattern -> procedural
// Urgency: "deploy", "production" -> important
// Domain: "Docker", "Kubectl", "production" -> technical
// Entities: ["Docker", "Kubectl"] extracted via keyword patterns
// Total pipeline: ~45ms (embedding: ~15ms, everything else: ~30ms)
// LLM calls: 0

// Compare with LLM-based classification:
// - Would require ~500ms-2000ms for GPT-4 API call
// - Would cost $0.001-0.01 per memory
// - Would produce slightly different output each time
// - Would fail if the API is down
```

---

### Feature 9: Security Filter

**What it does:** The security filter applies a 3-tier classification to every incoming memory: `public` (shared across the mesh), `private` (agent-scoped), or `secret` (blocked from storage entirely). It runs a battery of pattern-matching rules that detect sensitive content -- API keys, AWS credentials, private keys, database passwords, JWT tokens, SSH keys, and other secrets.

**How it works:** The filter maintains a library of regular expressions and pattern rules for common secret formats: AWS access keys (`AKIA...`), API keys (`sk-proj-...`, `sk-live-...`), private keys (`-----BEGIN.*PRIVATE KEY-----`), database connection strings with embedded passwords, JWT tokens, SSH private keys, and generic high-entropy strings that resemble tokens or secrets. When a secret pattern is matched, the memory is immediately rejected -- no vector is generated, no metadata is written, no trace of the secret enters the system. The caller receives a `blocked_secret` status. Private classification is applied based on content analysis (personal notes, debugging context, session-specific information) and explicit caller hints.

**Why it matters:** AI agents handle sensitive information constantly -- configuration files, deployment commands, database connections. Without a security filter, these secrets would be embedded, stored, and potentially retrieved by any agent in the mesh. The security filter is the first line of defense, and it is intentionally conservative: false positives (blocking a non-secret) are preferable to false negatives (storing a credential). A blocked memory can be reformulated without the sensitive content. A stored credential cannot be un-stored from the embedding space.

**Code example:**

```typescript
// Secrets are automatically blocked
const result1 = await m.store("The API key is sk-proj-abc123xyz789");
// result1: { status: "blocked_secret" }
// Nothing was stored. Nothing was embedded. The key never touched the database.

// AWS credentials blocked
const result2 = await m.store("AWS access key: AKIAIOSFODNN7EXAMPLE");
// result2: { status: "blocked_secret" }

// Private keys blocked
const result3 = await m.store("-----BEGIN RSA PRIVATE KEY-----\nMIIEow...");
// result3: { status: "blocked_secret" }

// Private information is agent-scoped
const result4 = await m.store("My internal debugging notes on the auth bug");
// Classified as private -> stored in agent-scoped collection
// Other agents cannot see this memory

// Safe reformulation works fine
const result5 = await m.store("Auth service uses API key authentication with sk-proj prefix");
// result5: { status: "created" } -- describes the pattern, not the actual key
```

---

### Feature 10: Smart Dedup & Merge

**What it does:** Before storing a new memory, the system searches for existing memories with high cosine similarity and operates in three distinct zones: duplicate (>=0.92), conflict (0.70-0.92), and novel (<0.70). Duplicates are merged intelligently; conflicts trigger alerts; novel memories proceed normally.

**How it works:** After embedding generation (step 2), the system queries Qdrant for the top-k most similar existing memories. The similarity score determines the action:

- **>= 0.92 (Duplicate):** The new memory is semantically identical to an existing one. A smart merge runs: the version with richer metadata, higher access count, or more recent timestamp is kept as the primary. Entity lists are unioned, linked memories are combined, and the access count is summed. The absorbed version is not created. The caller receives `duplicate` status.
- **0.70 - 0.92 (Conflict Zone):** The memories are related but not identical -- potentially an update, a contradiction, or complementary information. The system broadcasts a `memory:conflict` event containing both memory IDs and their similarity score. The consolidation engine (Layer 5) evaluates whether the new memory supersedes, contradicts, or supplements the existing one. Both memories are stored.
- **< 0.70 (Novel):** The memory is sufficiently distinct from everything in the collection. It proceeds through the pipeline as a new entry with no dedup action.

**Why it matters:** Without deduplication, conversational agents would rapidly fill the memory store with near-identical memories ("Redis is on port 6379", "Redis server runs on port 6379", "Redis configured on port 6379"). Each duplicate degrades search quality by diluting the result set with redundant information. The smart merge preserves the best version of each fact. The conflict zone catches updates and contradictions that need resolution -- a critical capability for maintaining knowledge accuracy over time.

**Code example:**

```typescript
// First store
await m.store("Redis is running on port 6379");
// status: "created"

// Exact semantic duplicate (different words, same meaning)
await m.store("Redis server is on port 6379");
// status: "duplicate" -- merged with existing, no new entry
// Access count summed, best metadata preserved

// Conflicting update
await m.store("Redis was moved to port 6380");
// status: "created" -- stored as new memory
// Conflict event broadcast: similarity 0.82 (in conflict zone)
// Consolidation will later flag this as potential contradiction

// Complementary information (below conflict zone)
await m.store("Redis uses the RESP protocol");
// status: "created" -- novel memory, different topic entirely
```

---

### Feature 11: Entity Extraction

**What it does:** The extraction pipeline pulls structured entities from unstructured memory text using purely algorithmic methods -- regular expressions, keyword matching, and contextual pattern detection. It identifies people, machines, IP addresses, port numbers, dates, technology names, URLs, and file paths.

**How it works:** Entity extraction runs as part of pipeline step 4. Each entity type has specialized extraction rules:
- **People:** Capitalization patterns (e.g., two consecutive capitalized words) combined with contextual cues ("deployed by", "asked", "reported")
- **IP Addresses:** IPv4/IPv6 patterns validated against network-context windows (near "server", "connect", "host", "ssh")
- **Ports:** Numeric values in port-context windows (near "port", "listen", "bind", `:` prefix)
- **Technologies:** Keyword dictionary of 200+ technology names (frameworks, databases, languages, tools)
- **Dates:** Both absolute ("February 20th", "2026-01-15") and relative ("last Tuesday", "yesterday", "3 days ago")
- **URLs:** HTTP/HTTPS patterns including partial domain references
- **File Paths:** Unix (`/etc/nginx/...`) and Windows (`C:\Users\...`) path patterns

The precision-first approach means the system may miss some entities (lower recall) but rarely produces false extractions (high precision). A missed entity can be caught on the next memory that mentions it, but a false entity pollutes the knowledge graph with noise.

**Why it matters:** Extracted entities are the atoms of the knowledge graph. Without entity extraction, the graph would be empty -- no nodes to connect, no relationships to traverse, no timelines to reconstruct. Every entity that is correctly extracted becomes a queryable node in the graph, linkable to other entities and traversable by the flash reasoning engine. The entities also enrich the memory's metadata payload, enabling filtered searches like "find all memories mentioning my-host" without requiring semantic similarity.

**Code example:**

```typescript
await m.store("my-host (192.168.1.50) is running Qdrant on port 6333 since February");
// Extracted entities:
// - "my-host" (machine name)
// - "192.168.1.50" (IP address, validated by network context)
// - "Qdrant" (technology, from dictionary)
// - "6333" (port number, near "port" keyword)
// - "February" (date, temporal reference)
// All entities added to memory.entities array and ingested into knowledge graph
```

---

### Feature 12: 7-Type Memory Taxonomy

**What it does:** Mnemosyne classifies every memory into one of 7 types using algorithmic pattern detection: episodic, semantic, preference, relationship, procedural, profile, and core. Each type carries specific behavioral properties that influence storage routing, retrieval ranking, activation decay, and consolidation behavior.

**How it works:** Classification runs during pipeline step 4 using pattern matching:

| Type | Description | Decay Behavior | Detection Patterns |
|------|-------------|----------------|-------------------|
| `episodic` | Specific events and experiences | Normal decay | Temporal markers ("yesterday", "last week"), event language ("happened", "occurred") |
| `semantic` | General knowledge and facts | Normal decay | Declarative statements, definitions, factual assertions |
| `preference` | User or agent preferences | Normal decay | "prefer", "always use", "like", "want", "dislike", "favorite" |
| `relationship` | Connections between entities | Normal decay | "works with", "reports to", "connected to", "depends on" |
| `procedural` | Step-by-step procedures and skills | **Immune to decay** | "step 1", "how to", "run the command", sequential instructions |
| `profile` | Agent or entity profile summaries | Normal decay | Profile-structured content, capability descriptions |
| `core` | Verified, high-value foundational memories | **Immune to decay** | Promoted via reinforcement learning or set by mesh sync blocks |

**Why it matters:** The type system connects classification to behavior. Procedural and core memories never decay -- modeling how human procedural memory (riding a bike) persists indefinitely while episodic memory (what you had for breakfast) naturally fades. The type also feeds into the multi-signal scoring system: a procedural query boosts procedural memories, a preference query boosts preference memories. Without the type system, the cognitive layer would treat all memories identically, losing the behavioral nuance that makes retrieval context-sensitive.

**Code example:**

```typescript
// Procedural memory is automatically detected and routed to skill library
await m.store("To deploy to production: 1) Run tests 2) Build Docker image 3) Push to registry 4) Kubectl apply");
// Type: procedural -- immune to decay, stored in skill library, fleet-wide shared

// Preference memory
await m.store("User prefers TypeScript over JavaScript for new projects");
// Type: preference -- boosted when intent is detected as preference query

// Core memories are created through promotion (reinforcement learning)
// or explicitly via mesh sync blocks
await m.blockSet("fleet_standard", "All services must use TLS 1.3");
// Stored as core memory with maximum confidence
```

---

## Layer 3: Knowledge Graph (5 Features)

The knowledge graph layer builds a temporal entity network on top of the flat memory store. While vector search finds memories by semantic similarity, the knowledge graph finds memories by structural relationships. This layer is implemented on FalkorDB and is optional; all other layers function without it, but enabling it unlocks capabilities that vector search alone cannot provide.

---

### Feature 13: Temporal Queries

**What it does:** The knowledge graph supports time-scoped queries that answer "What was entity X connected to as of date Y?" by leveraging the `since` timestamps on every relationship edge. This enables both point-in-time snapshots and change-over-time analysis.

**How it works:** Every `RELATES_TO` edge in the graph carries a `since` timestamp derived from the memory's `eventTime`. Temporal queries filter edges by this timestamp, returning only relationships that existed at the specified point in time. The bi-temporal model means you can query either the real-world timeline (using `eventTime`-derived `since` values) or the knowledge-acquisition timeline (using `ingestedAt`-derived values). FalkorDB's Cypher query language supports these temporal predicates natively through WHERE clauses on edge properties.

**Why it matters:** Static graphs answer "What is true now?" Temporal graphs answer "What was true then?" and "What changed between then and now?" This is invaluable for incident investigation (what changed in the last 48 hours?), project tracking (what was the team composition last quarter?), and audit compliance (what did the system believe at the time of decision X?).

**Code example:**

```typescript
// The graph grows automatically as memories are stored
await m.store("my-host hosts Qdrant since January 2026");
await m.store("my-host also runs Redis since February 2026");

// Temporal graph query: What was running on my-host as of January 15th?
// Cypher: MATCH (e:Entity {name:'my-host'})-[r:RELATES_TO]->(s:Entity)
//         WHERE r.since <= '2026-01-15' RETURN s
// Result: [Qdrant] -- Redis hadn't been deployed yet

// Current state query: What is running on my-host now?
// Result: [Qdrant, Redis]

// Change detection: What changed on my-host in February?
// Result: [Redis was added]
```

---

### Feature 14: Auto-Linking (Bidirectional, Zettelkasten)

**What it does:** During pipeline step 10, every new memory is compared against existing memories, and bidirectional links are created for any pair exceeding the 0.70 cosine similarity threshold. This creates a Zettelkasten-style knowledge web where related memories are directly connected and traversable.

**How it works:** After a memory is stored in vector storage (step 9), the system queries for existing memories with cosine similarity > 0.70 to the new memory's embedding. For each qualifying match, two operations occur: (1) the new memory's `linkedMemories` array gains the existing memory's ID, and (2) the existing memory's `linkedMemories` array gains the new memory's ID. Links are stored both in the vector database (as part of the metadata payload) and in the graph database (as traversable edges between memory nodes). The bidirectional nature ensures traversal works in either direction -- from a conclusion back to its supporting evidence, or from evidence forward to its implications.

**Why it matters:** Without auto-linking, the memory store is a flat collection of independent vectors. Finding connections requires running new searches from scratch. With auto-linking, the structure of knowledge is explicit and traversable. The flash reasoning engine (Layer 5) exploits these links to build multi-step reasoning chains that connect disparate pieces of information. The Zettelkasten metaphor is apt: just as a Zettelkasten's value grows super-linearly with the number of notes because of the connections between them, Mnemosyne's knowledge value grows super-linearly with the number of memories because of auto-linking.

**Code example:**

```typescript
await m.store("Qdrant is our vector database");
await m.store("Qdrant runs on port 6333 on my-host");
await m.store("my-host is a dedicated host at 192.168.1.50");

// These three memories are now bidirectionally linked:
// "Qdrant is our vector database" <-> "Qdrant runs on port 6333 on my-host"
// "Qdrant runs on port 6333 on my-host" <-> "my-host is a dedicated host..."

// Flash reasoning can now traverse:
// Qdrant -> runs on my-host -> is at 192.168.1.50

// The 0.70 threshold is configurable:
// Lower (0.60) = denser links, more tangential connections
// Higher (0.80) = sparser links, only strongly related memories
```

---

### Feature 15: Path Finding

**What it does:** The knowledge graph supports shortest-path queries between any two entities, with a configurable maximum depth to prevent unbounded traversal. Path finding answers "How is entity A related to entity B?" by discovering the chain of entities that connect them.

**How it works:** Path queries execute as Cypher shortest-path queries against FalkorDB: `MATCH path = shortestPath((a:Entity {name:$start})-[*..N]-(b:Entity {name:$end})) RETURN path` where N is the configurable max depth (default: 3). The result is a sequence of entities and relationship edges forming the shortest connection between the two endpoints. If no path exists within the depth limit, the query returns empty. Multiple paths of equal length are all returned, providing alternative explanations for the relationship.

**Why it matters:** Path finding reveals indirect relationships that vector search cannot surface. A direct vector search for "how is Alice related to PostgreSQL" might not find a connection if no single memory mentions both. But a graph path query can discover: Alice -> deployed auth service -> auth service uses -> PostgreSQL -- a 3-hop path that reveals the relationship through intermediate entities. This is essential for answering "why" and "how" questions about complex systems.

**Code example:**

```typescript
// Path finding discovers indirect relationships
const path = await m.findPath("Alice", "PostgreSQL", { maxDepth: 4 });
// Result: Alice --deployed--> auth-service --uses--> PostgreSQL
// This connection exists even though no single memory mentions both endpoints

// Multiple paths may exist
const paths = await m.findPath("Redis", "auth-service", { maxDepth: 3 });
// Path 1: Redis --caches--> sessions --used_by--> auth-service
// Path 2: Redis --hosts--> JWT-store --used_by--> auth-service
```

---

### Feature 16: Timeline Reconstruction

**What it does:** Given any entity in the knowledge graph, Mnemosyne reconstructs a chronologically ordered timeline of all memories that mention that entity. This provides a complete history of everything the system knows about that entity, ordered by when events occurred or when they were learned.

**How it works:** Timeline reconstruction queries the graph for all `Memory --MENTIONS--> Entity` edges targeting the specified entity, then retrieves the corresponding memory cells from the vector store, and finally sorts them by `eventTime` (for a real-world timeline) or `ingestedAt` (for a knowledge-acquisition timeline). The result is a chronological sequence of memories, each with its full metadata payload, providing a narrative view of the entity's history.

**Why it matters:** Timeline reconstruction answers "tell me the complete history of X" -- a question that vector search can only approximate (it would return the most similar memories, not necessarily the most chronologically relevant ones). For incident investigation, project tracking, and audit trails, chronological ordering is essential for understanding causality and progression.

**Code example:**

```typescript
// Reconstruct the timeline for an entity
const timeline = await m.timeline("my-database");
// Returns chronologically ordered:
// 2026-01-15: "Migrated my-database from PostgreSQL 14 to 16"
// 2026-01-20: "my-database replication lag increased to 500ms"
// 2026-02-01: "Resolved replication lag by upgrading instance size"
// 2026-02-15: "my-database backup schedule changed to every 6 hours"
// 2026-02-22: "my-database connection pool increased to 200"
```

---

### Feature 17: Depth-Limited Traversal

**What it does:** All graph traversal operations (path finding, flash reasoning, entity expansion) are bounded by a configurable maximum depth to prevent unbounded exploration and maintain predictable query performance. The default depth is 2 hops for general traversal, 3 hops for path finding.

**How it works:** The depth limit is enforced at the Cypher query level using variable-length relationship patterns (`[*..N]`). At depth 1, traversal only returns directly connected entities. At depth 2, it includes one-hop-removed connections. At depth 3, it includes two-hop-removed connections. Beyond depth 3, the number of reachable nodes grows combinatorially, and the relationships become increasingly tenuous. The depth limit can be adjusted per-query for domains where deeper traversal is meaningful (e.g., supply chain analysis where 5-hop connections are still relevant).

**Why it matters:** Without depth limiting, graph traversal on a densely connected knowledge graph could explore thousands of nodes, consume excessive computation time, and return results where the connection to the original query is so tenuous as to be meaningless. The depth limit ensures bounded latency and relevance. The default of 2-3 hops balances discovery of non-obvious connections against noise from overly distant relationships.

**Code example:**

```typescript
// Depth 1: Only direct connections
const direct = await m.graphExpand("my-host", { maxDepth: 1 });
// Returns: [Qdrant, Redis, Alice] -- only entities directly related to my-host

// Depth 2: One hop removed
const nearby = await m.graphExpand("my-host", { maxDepth: 2 });
// Returns: [Qdrant, Redis, Alice, auth-service, PostgreSQL, ...]
// Now includes entities related to my-host's direct connections

// Depth 3 (default for path finding)
const extended = await m.graphExpand("my-host", { maxDepth: 3 });
// Returns a much larger set -- use with caution on dense graphs
```

---

## Layer 4: Cognitive (6 Features)

The cognitive layer transforms Mnemosyne from a storage-and-retrieval system into an intelligent memory architecture that adapts to context, models temporal relevance, and ensures diversity in its outputs. These features operate during the recall path, taking raw vector search results and refining them through multiple stages of scoring, filtering, and reranking.

---

### Feature 18: Activation Decay

**What it does:** Memories have activation levels that decay over time following a logarithmic model inspired by the Ebbinghaus forgetting curve. Each access refreshes activation. Configurable decay rates per urgency level ensure that critical memories persist while background observations naturally fade.

**How it works:** The activation level A(t) is computed as: `A(t) = baseline - rate * ln(1 + hours_since_last_access)` where `baseline` and `rate` are determined by the memory's urgency level. The activation level maps to three states:

| Urgency | Decay Rate | Baseline | Practical Effect |
|---------|-----------|----------|-----------------|
| Critical | 0.3 (slowest) | +2.0 | Remains active for months without access |
| Important | 0.5 | +1.0 | Remains active for weeks |
| Reference | 0.6 | 0.0 | Fades over days |
| Background | 0.8 (fastest) | -1.0 | Fades within hours |

Activation states: **Active** (>= -2.0) -- fully retrievable. **Fading** (-2.0 to -4.0) -- retrievable with score penalty. **Archived** (< -4.0) -- excluded from search. Two memory types are completely immune: `core` and `procedural` maintain permanent Active status.

**Why it matters:** Without decay, the memory store would grow monotonically, and old, irrelevant memories would compete equally with fresh, relevant ones for retrieval slots. Activation decay naturally surfaces recent and frequently-accessed memories while allowing low-value observations to gracefully fade. The urgency-based rates ensure that critical operational knowledge persists for months while casual observations fade within hours -- matching human memory behavior where important events are remembered longer than trivial ones.

**Code example:**

```typescript
// Critical memory stays active for months
await m.store("CRITICAL: Never deploy to prod on Fridays -- caused outage 2026-01-10", {
  importance: 1.0
});
// Urgency: critical, decay rate: 0.3, baseline: +2.0
// After 90 days without access: A = 2.0 - 0.3 * ln(1 + 2160) = 2.0 - 2.3 = -0.3 (still Active)

// Background observation fades quickly
await m.store("User mentioned they had coffee this morning");
// Urgency: background, decay rate: 0.8, baseline: -1.0
// After 24 hours: A = -1.0 - 0.8 * ln(1 + 24) = -1.0 - 2.57 = -3.57 (Fading)
// After 72 hours: A = -1.0 - 0.8 * ln(1 + 72) = -1.0 - 3.43 = -4.43 (Archived)

// Accessing a memory refreshes its activation
const results = await m.recall("deployment rules");
// The "never deploy on Friday" memory is accessed -> activation refreshed to baseline +2.0

// Core and procedural types are IMMUNE
await m.store("To restart auth: ssh my-server, systemctl restart my-auth");
// Type: procedural -- activation permanently Active, never decays
```

---

### Feature 19: Multi-Signal Scoring

**What it does:** Every vector search result is scored across 5 independent signals, producing a composite relevance score far more nuanced than raw cosine similarity. The signals measure semantic similarity, temporal recency, importance weighted by confidence, access frequency, and type relevance.

**How it works:** The 5 signals are:

| Signal | Baseline Weight | What It Measures |
|--------|----------------|-----------------|
| **Semantic Similarity** | 35% | Cosine distance between query and memory embeddings |
| **Temporal Recency** | 20% | Time-decay function from most recent access or ingestion timestamp |
| **Importance x Confidence** | 20% | Product of priority score (urgency x domain) and confidence score |
| **Access Frequency** | 15% | Logarithmic function of `accessCount` -- rewards consistent utility |
| **Type Relevance** | 10% | Match quality between memory type and inferred query intent |

The composite score is a weighted sum: `score = w1*similarity + w2*recency + w3*(importance*confidence) + w4*log(1+frequency) + w5*typeMatch`. These baseline weights are dynamically adjusted by the intent-aware retrieval system (Feature 20).

**Why it matters:** Raw cosine similarity is a single signal that captures only semantic relatedness. A memory that is semantically close but outdated, low-confidence, and never accessed is almost certainly less useful than one that is slightly less similar but recent, high-confidence, and frequently retrieved. Multi-signal scoring captures these nuances, producing rankings that align with actual utility rather than just vector distance.

**Code example:**

```typescript
const results = await m.recall("how do we deploy to production?");
// Each result has a composite score from all 5 signals:
// {
//   text: "To deploy: 1) Run tests 2) Build 3) Push 4) Apply",
//   score: 0.92,                    // Composite of all 5 signals
//   signals: {
//     similarity: 0.85,             // Strong semantic match
//     recency: 0.60,                // Stored 2 weeks ago
//     importanceConfidence: 0.76,   // High priority x grounded confidence
//     frequency: 0.80,              // Retrieved 8 times before (log scale)
//     typeRelevance: 0.95,          // Procedural type matches procedural intent
//   }
// }
```

---

### Feature 20: Intent-Aware Retrieval

**What it does:** Mnemosyne automatically detects the intent behind every recall query and adjusts the multi-signal scoring weights accordingly. Five intents are recognized: factual, temporal, procedural, preference, and exploratory. No user configuration or explicit intent specification is needed.

**How it works:** Intent detection analyzes the query text for linguistic patterns:

| Intent | Detection Patterns | Primary Weight Adjustment |
|--------|-------------------|--------------------------|
| **Factual** | "what is", "how many", "which", definition-style queries | Similarity weight -> 50% |
| **Temporal** | "when", "last time", "recently", "history", "timeline" | Recency weight -> 35% |
| **Procedural** | "how to", "steps to", "procedure for", "guide to" | Frequency +20%, type relevance boost for procedural |
| **Preference** | "prefer", "favorite", "style", "do they like" | Type relevance +20% for preference type |
| **Exploratory** | Open-ended queries, "tell me about", "what do we know about" | All weights balanced evenly |

The weight adjustment is applied as a modifier to the baseline weights in the multi-signal scoring function. This means the same set of candidate memories can produce different rankings depending on the detected intent -- a factual query surfaces the most semantically similar memories, while a temporal query surfaces the most recent ones.

**Why it matters:** Intent-aware retrieval prevents a common failure mode: a temporal query like "what happened recently?" returning old but highly similar memories, or a procedural query returning factual definitions instead of step-by-step guides. By adapting the scoring weights to the intent, the system produces results that match what the user actually wants, not just what is most similar.

**Code example:**

```typescript
// Factual intent: similarity dominates
const fact = await m.recall("what port does Qdrant use?");
// Weights: similarity 50%, recency 15%, importance*conf 15%, frequency 10%, type 10%
// Returns: Most semantically similar factual memories about Qdrant ports

// Temporal intent: recency dominates
const recent = await m.recall("what happened with the database recently?");
// Weights: similarity 25%, recency 35%, importance*conf 15%, frequency 15%, type 10%
// Returns: Most recent database-related memories, even if slightly less similar

// Procedural intent: frequency and type relevance boosted
const howto = await m.recall("how do I restart the auth service?");
// Weights: similarity 30%, recency 10%, importance*conf 15%, frequency 25%, type 20%
// Returns: Frequently-used procedural memories about auth service restarts
```

---

### Feature 21: Diversity Reranking

**What it does:** After multi-signal scoring produces a ranked list of results, a final diversity reranking pass ensures maximum information value rather than maximum similarity. It applies three penalties to prevent redundant results and encourage variety across memory types and semantic clusters.

**How it works:** Three penalty mechanisms:

1. **Cluster Detection (>0.9 similarity, -40% penalty):** Results more than 0.9 similar to each other are grouped into clusters. Only the highest-scoring cluster member passes through at full score. Others receive -40% penalty.

2. **Overlap Penalty (>0.8 similarity, -15% penalty):** Results more than 0.8 similar to any already-selected result receive -15% penalty. Softer than cluster detection -- the memories are related but not duplicative.

3. **Type Diversity (-5% per 3+ of same type):** When the same memory type appears 3+ times in results, each additional instance receives -5% penalty. Encourages a mix of episodic, semantic, procedural, preference, and other types.

**Why it matters:** Without diversity reranking, a query about "deployment" might return five slightly different versions of the same deployment guide -- high similarity but low information value. With it, the results include a deployment guide, a recent deployment event, an error from a past deployment, a configuration preference, and a related decision. The agent gets multiple perspectives instead of redundant repetitions.

**Code example:**

```typescript
const results = await m.recall("nginx configuration");
// Without diversity reranking:
// 1. "nginx config for production load balancer" (0.92)
// 2. "nginx production lb configuration" (0.91)    -- near-duplicate!
// 3. "nginx configuration for production" (0.90)    -- near-duplicate!
// 4. "nginx proxy settings for prod" (0.88)          -- near-duplicate!
// 5. "nginx ssl certificate setup" (0.82)

// With diversity reranking:
// 1. "nginx config for production load balancer" (0.92 -- cluster winner)
// 2. "nginx ssl certificate setup" (0.82 -- different topic)
// 3. "Last nginx config change caused 502 errors" (0.78 -- episodic, different type)
// 4. "Team prefers nginx over HAProxy" (0.72 -- preference, different type)
// 5. "nginx production lb configuration" (0.55 -- cluster member, -40% penalty)
```

---

### Feature 22: 4-Tier Confidence System

**What it does:** Every memory carries a confidence score computed from three independent signals (retrieval quality, cross-agent agreement, source trust) that maps to one of four human-readable tiers: Mesh Fact, Grounded, Inferred, and Uncertain.

**How it works:** Confidence is a weighted composite of three signals:

- **Retrieval Quality (50% weight):** Measures how cleanly the original information was captured. Concise, unambiguous factual statements score high. Vague, fragmentary statements score low. Assessed algorithmically based on text length, structural clarity, and entity extraction yield.
- **Cross-Agent Agreement (30% weight):** When multiple agents independently store corroborating memories, agreement rises. This is the most powerful signal because independent corroboration is inherently difficult to game. When 3+ agents agree, confidence reaches the Mesh Fact tier.
- **Source Trust (20% weight):** Configurable trust hierarchy for different agents. A production monitoring agent might have trust 0.95; an experimental agent might have 0.50.

The composite score maps to tiers:

| Tier | Score Range | Meaning |
|------|-------------|---------|
| **Mesh Fact** | >= 0.85 | Corroborated by multiple agents. Highest reliability. |
| **Grounded** | 0.65 - 0.84 | Strong single-source evidence. |
| **Inferred** | 0.40 - 0.64 | Reasonable inference, not directly verified. |
| **Uncertain** | < 0.40 | Low confidence, should be verified. |

**Why it matters:** Not all memories are equally reliable. A fact corroborated by 5 independent agents is more trustworthy than a vague inference from a single experimental agent. The confidence system makes this distinction explicit and actionable -- agents can check the confidence tag before acting on a recalled memory, and the multi-signal scoring system weights high-confidence memories higher than uncertain ones.

**Code example:**

```typescript
const memories = await m.recall("what is the database port");
// Each result includes confidence:
// {
//   text: "PostgreSQL runs on port 5432 on example-db",
//   confidenceScore: 0.88,
//   confidenceTag: "Mesh Fact",     // 3+ agents corroborated this
// }
// {
//   text: "I think the test DB might be on port 5433",
//   confidenceScore: 0.35,
//   confidenceTag: "Uncertain",     // Single source, vague language
// }
```

---

### Feature 23: Priority Scoring

**What it does:** Priority is computed as a composite score from 0.0 to 1.0 by combining urgency (4 levels) and domain (5 categories). The score provides a stable, precomputed ranking signal that the cognitive layer blends with dynamic signals during retrieval.

**How it works:** The urgency x domain matrix:

| | Technical | Personal | Project | Knowledge | General |
|---|---|---|---|---|---|
| **Critical** | 1.0 | 0.9 | 0.95 | 0.85 | 0.8 |
| **Important** | 0.8 | 0.7 | 0.75 | 0.65 | 0.6 |
| **Reference** | 0.5 | 0.4 | 0.45 | 0.4 | 0.35 |
| **Background** | 0.3 | 0.25 | 0.25 | 0.25 | 0.2 |

Priority scoring runs at pipeline step 7, after urgency (step 5) and domain (step 6) classification. The score is stored in the memory's `priorityScore` field and used by the multi-signal scoring system as a component of the Importance x Confidence signal.

**Why it matters:** Priority scoring decouples importance assessment from retrieval-time computation. By precomputing the priority during ingestion, the cognitive layer avoids expensive re-evaluation during recall. The urgency x domain matrix captures the intuition that a critical technical issue (priority 1.0) is more actionable than a background general observation (priority 0.2), and that this distinction should influence retrieval ranking.

**Code example:**

```typescript
await m.store("CRITICAL: Production database is down on example-db");
// Urgency: critical, Domain: technical -> Priority: 1.0

await m.store("User mentioned they like dark mode");
// Urgency: background, Domain: personal -> Priority: 0.25

// During recall, priority feeds into multi-signal scoring:
// The database outage (priority 1.0) ranks higher than the theme preference (0.25)
// even if both are equally similar to the query
```

---

## Layer 5: Self-Improvement (10 Features)

The self-improvement layer enables Mnemosyne to learn from its own operation, maintain its own health, reason across linked memories, model other agents' knowledge, and survive context window resets. These features transform a memory system from a passive data store into an active cognitive participant in the agent's reasoning process.

---

### Feature 24: Reinforcement Learning

**What it does:** A closed-loop feedback system that tracks which memories actually proved useful to agents and adjusts their standing accordingly. Memories that consistently help are promoted to core status with permanent retention. Memories that consistently mislead are flagged for review.

**How it works:** After each recall-and-response cycle, the system monitors for positive and negative signals:

- **Positive signals:** explicit thanks ("that's right", "exactly"), agent referencing recalled content, successful task completion, explicit `memory_feedback("positive")`
- **Negative signals:** corrections ("that's wrong", "no, it's actually..."), agent ignoring recalled memories, explicit `memory_feedback("negative")`

Each memory tracks: `hit_count` (total retrievals), `useful_count` (positive feedback count), `usefulness_ratio` (useful_count / hit_count).

**Promotion rule:** When `usefulness_ratio > 0.7` after `3+ retrievals`, the memory is automatically promoted to `core` type -- immune to decay, highest confidence tier.

**Review flagging:** When `usefulness_ratio < 0.3` after `3+ retrievals`, the memory is flagged for review.

**Why it matters:** Without reinforcement learning, memory quality is static -- a memory stored with poor phrasing or incorrect information remains at the same standing forever. With it, the system self-corrects: good memories rise to the top (core, permanent retention), and bad memories are identified and flagged. Over time, the retrieval quality of the entire system improves without any manual curation.

**Code example:**

```typescript
// Explicit feedback
await m.recall("database connection string");
// Agent uses the recalled memory successfully...
await m.feedback("positive");
// hit_count: 4 -> 5, useful_count: 3 -> 4, usefulness_ratio: 0.80
// 3+ retrievals and ratio > 0.7 -> PROMOTED to core memory type
// Now immune to decay, confidence elevated

// Automatic feedback detection (via agent_end lifecycle hook)
// User: "No, that's wrong -- the port is 5433, not 5432"
// System detects correction -> negative signal applied
// hit_count: 6, useful_count: 1, usefulness_ratio: 0.17 -> FLAGGED for review
```

---

### Feature 25: Active Consolidation

**What it does:** A 4-phase autonomous maintenance pipeline that detects contradictions, merges near-duplicates, promotes popular memories, and demotes stale ones. Runs on-demand via `memory_consolidate` or on a schedule. No human intervention required.

**How it works:** Four phases execute in sequence:

**Phase 1 -- Contradiction Detection:** Scans for memory pairs in the 0.70-0.92 similarity range and applies semantic conflict analysis. Memories that are highly similar but assert contradictory facts are flagged. In clear-cut cases (one has a much more recent `eventTime`), the system auto-resolves in favor of the newer version.

**Phase 2 -- Near-Duplicate Merge:** Memories exceeding 0.92 similarity that were not caught during initial ingestion are merged. The merge preserves the version with higher access count, more complete metadata, and more recent timestamp. The absorbed version is soft-deleted with a reference to the survivor.

**Phase 3 -- Popular Promotion:** Memories with more than 10 accesses and a healthy usefulness ratio are promoted to `core` type. Sheer retrieval frequency is itself a signal of value.

**Phase 4 -- Stale Demotion:** Memories idle for 30+ days with importance below 0.3 have their priority score reduced. Not deletion -- just deprioritization. Keeps the active knowledge base sharp.

**Why it matters:** Memory stores degrade over time: duplicates accumulate, contradictions arise as facts change, popular knowledge remains at the same priority as rarely-used trivia, and stale memories compete with fresh ones. Active consolidation is autonomous cognitive hygiene -- the system maintains its own health.

**Code example:**

```typescript
const report = await m.consolidate({ batchSize: 200 });
// Returns:
// {
//   contradictions: 3,        // 3 contradiction pairs found and flagged
//   nearDuplicatesMerged: 7,  // 7 duplicate pairs merged (7 soft-deleted)
//   popularPromoted: 2,       // 2 memories promoted to core
//   staleDemoted: 15          // 15 idle memories deprioritized
// }

// CLI: mnemosyne consolidate-deep --batch 200
// Dry run: mnemosyne consolidate --dry-run
```

---

### Feature 26: Flash Reasoning

**What it does:** A BFS (breadth-first search) traversal engine that walks `linkedMemories` connections between memories to construct multi-step reasoning chains. When a memory is recalled, flash reasoning expands outward from it, discovering related memories that add context, causality, or consequence.

**How it works:** Starting from a recalled memory, the engine performs BFS traversal through the `linkedMemories` graph. At each step, it evaluates the relationship between the current memory and the linked memory, inferring a relationship type:

- **`leads_to`**: Memory A chronologically precedes B, and B's entities overlap with A's
- **`because`**: Memory B provides a causal explanation for A
- **`therefore`**: Memory A leads to a conclusion expressed in B
- **`related_to`**: Memories share entities or themes without clear directionality

Cycle detection prevents infinite traversal (visited set). Depth limiting (configurable, default 3 hops) bounds the expansion. The resulting chains are appended to recall results as contextual enrichment.

**Why it matters:** Flash reasoning gives agents narrative context, not just isolated facts. An agent recalling a memory about a service failure receives the full chain: what caused it, what was done about it, and what the outcome was. This eliminates the need for multiple follow-up queries and enables the agent to reason about causality and consequences from a single recall.

**Code example:**

```typescript
const results = await m.recall("why did auth service crash last week?");
// Primary result: "Auth service crashed on 2026-02-18 after config update"
// Flash reasoning chain:
// -> (because) "Config update changed JWT expiry from 1hr to 30min"
// -> (leads_to) "Short-lived tokens caused session storm"
// -> (therefore) "Rollback to 1hr expiry resolved the issue"
// -> (related_to) "Team decided to never change token expiry without load testing"
// The agent gets the complete narrative from a single recall
```

---

### Feature 27: Agent Awareness / Theory of Mind for Agents (ToMA)

**What it does:** Enables any agent in the mesh to model what other agents know. Agents can query another agent's knowledge on a topic, analyze knowledge gaps between agents, and maintain aggregated agent profiles -- all without direct inter-agent communication.

**How it works:** Four sub-capabilities:

**Agent Knowledge Query:** Filters the shared memory store for memories created by a specific agent (`botId` filter) and semantically matches the topic. Returns that agent's knowledge on the topic, ranked by relevance and recency.

**Knowledge Gap Analysis:** Given two agents and a topic, identifies what each knows that the other does not. Performs vector search filtered by each agent's `botId`, then compares result sets to surface asymmetric knowledge.

**Agent Profiles:** Aggregated summaries stored in the Agent Profiles collection: total memory count, top domains, top memory types, average confidence, last active timestamp. Periodically refreshed.

**Auto-Detection:** Queries mentioning agent names are automatically routed through the ToMA engine. "What does the DevOps agent know about Redis?" is transformed into a filtered query without requiring explicit API calls.

**Why it matters:** In a multi-agent system, knowing what other agents know is essential for intelligent task routing, collaboration, and avoiding redundant work. Without ToMA, agents operate as isolated knowledge silos. With it, any agent can leverage the entire fleet's collective knowledge by understanding who knows what.

**Code example:**

```typescript
// Direct ToMA query
const devopsKnowledge = await m.toma("agent-devops", "production database");
// Returns: Agent-DevOps's memories about the production database

// Knowledge gap analysis
const gap = await m.knowledgeGap("agent-frontend", "agent-backend", "API contracts");
// Returns: {
//   onlyFrontendKnows: ["API uses camelCase response format", ...],
//   onlyBackendKnows: ["Rate limit is 1000 req/min per client", ...],
//   bothKnow: ["Auth endpoint is /api/v2/auth", ...]
// }

// Auto-detection in regular recall
const results = await m.recall("what does the monitoring agent know about alerts?");
// Automatically routed through ToMA engine
```

---

### Feature 28: Cross-Agent Synthesis

**What it does:** When 3 or more agents independently store memories that corroborate the same fact, the engine automatically synthesizes a fleet-level insight with elevated confidence. This transforms independent observations into verified fleet knowledge.

**How it works:** The synthesis engine monitors new memory events via the broadcast system. When a new memory is stored, the system checks for existing memories from different agents with cosine similarity >= 0.85 (strong agreement). When 3+ agents have corroborating memories on the same topic, a synthesis operation fires: the system generates a consolidated memory with the `Mesh Fact` confidence tier, marks it as cross-agent validated, and stores it with maximum priority. The source memories are linked to the synthesized memory for traceability. The threshold of 3 agents balances sensitivity against false positives -- independent corroboration from 3 separate sources operating in different contexts is strong evidence of factual accuracy.

**Why it matters:** Individual agents may observe partial truths or make uncertain inferences. Cross-agent synthesis elevates observations that multiple agents independently confirm into verified fleet knowledge. This is a form of distributed consensus applied to knowledge management -- the fleet collectively validates facts that no single agent could confirm alone.

**Code example:**

```typescript
// Agent A stores: "PostgreSQL is on port 5432 on example-db"
// Agent B stores: "The database server example-db runs PostgreSQL on 5432"
// Agent C stores: "example-db hosts PostgreSQL, port 5432"

// 3 agents agree -> Cross-agent synthesis fires
// Synthesized memory created:
// text: "PostgreSQL runs on port 5432 on example-db"
// confidenceTag: "Mesh Fact"
// confidenceScore: 0.92
// source: "cross-agent-synthesis"
// Linked to all 3 source memories for traceability
```

---

### Feature 29: Proactive Recall

**What it does:** Before every agent invocation, Mnemosyne generates speculative queries based on the incoming prompt, retrieves contextually relevant memories, and injects them as pre-loaded context. The agent starts every conversation with relevant history already available, eliminating the cold-start problem.

**How it works:** The `before_agent_start` lifecycle hook performs three operations:

1. **Session Recovery:** Checks for a session snapshot from a previous context compaction. If found, deserializes and injects it.
2. **Speculative Query Generation:** Analyzes the incoming user prompt and generates 1-3 speculative queries that might be relevant. For "deploy the auth service", it generates: "auth service deployment procedure", "recent auth service issues", "auth service configuration".
3. **Context Injection:** Recalled memories from speculative queries, plus any recovered session state, are prepended to the agent's context.

**Why it matters:** The difference between a stateless agent and a Mnemosyne-equipped agent is the difference between "I don't have context about your nginx setup" and "Based on the current nginx config on my-server and the issue we had last time with the upstream timeout, here's what I recommend..." Proactive recall makes agents appear to have continuous awareness, even though they are technically stateless between invocations.

**Code example:**

```typescript
// This happens automatically before every agent invocation
// User prompt: "Let's update the nginx config for the load balancer"

// Proactive recall generates:
// Query 1: "nginx load balancer configuration" -> finds current config memory
// Query 2: "recent nginx changes" -> finds last config change and its outcome
// Query 3: "nginx configuration procedures" -> finds the procedural memory

// All three results are injected as context BEFORE the agent sees the user prompt
// The agent starts with full awareness of current config, recent changes, and procedures
```

---

### Feature 30: Session Survival

**What it does:** Ensures cognitive continuity across context window resets. When an agent's context window fills up and is compacted, Mnemosyne captures a structured snapshot of the active cognitive state and restores it on the next invocation. The agent experiences no discontinuity.

**How it works:** Two lifecycle hooks:

**`agent_end` -- Snapshot Capture:** At the end of every invocation, captures: recent recalled memories, decisions made, open conversation threads, and current task context. Stored as a private memory in the agent's scoped collection.

**`before_agent_start` -- Snapshot Recovery:** At the start of every invocation, checks for a recent snapshot from the same agent. If the agent's context was compacted, the snapshot is recovered and injected as pre-loaded context.

**Why it matters:** Without session survival, every context compaction is a hard reset -- the agent loses all accumulated context and starts from scratch. Long-running conversations about complex topics (database migrations, multi-step debugging) would be repeatedly interrupted by context resets. Session survival makes these resets invisible to both the agent and the user.

**Code example:**

```typescript
// Session 1: Long conversation about database migration
// ... 50 turns later, context window compacts ...

// Session 2 (automatic):
// before_agent_start fires -> finds session snapshot
// Injects: "Previous session context: Working on PostgreSQL 14->16 migration.
//           Completed: schema validation, data backup.
//           In progress: running migration script on staging.
//           Open threads: need to verify replication after migration."
// Agent continues seamlessly: "Let me check on the staging migration status..."
```

---

### Feature 31: Observational Memory

**What it does:** Compresses raw conversational streams into structured, high-signal memory cells. Rather than storing every utterance, the system identifies salient facts, decisions, and behavioral patterns and consolidates them into compact memories. Typical compression ratio: 10:1 or better.

**How it works:** During the `agent_end` lifecycle hook, the system analyzes the completed conversation and auto-captures up to 3 noteworthy memories. The selection algorithm prioritizes:
1. Explicit decisions ("We decided to use PostgreSQL")
2. Factual discoveries ("The API rate limit is 1000/min")
3. Behavioral patterns ("User always asks for TypeScript examples")
4. Procedural knowledge ("The deploy script requires the VPN to be connected first")

Each captured observation passes through the full 12-step pipeline, receiving proper classification, scoring, linking, and graph ingestion.

**Why it matters:** Storing every conversational utterance would quickly overwhelm the memory store with low-value content ("ok", "thanks", "let me think about that"). Observational memory mimics how human working memory selectively encodes experiences into long-term storage -- capturing what matters, discarding noise. A 20-turn conversation produces 2-3 high-signal memories instead of 20+ noisy ones.

**Code example:**

```typescript
// After a 20-turn conversation about debugging a Redis connection issue:
// Observational memory auto-captures:
// 1. "Redis connection timeout was caused by firewall rule on port 6379" (semantic)
// 2. "To fix Redis firewall: ssh into gateway, run iptables -A INPUT -p tcp --dport 6379 -j ACCEPT" (procedural)
// 3. "User prefers debugging with redis-cli over Redis Insight" (preference)
// Only 3 high-signal memories from 20 turns of conversation
```

---

### Feature 32: Procedural Memory / Skill Library

**What it does:** Learned multi-step procedures are stored as first-class memory objects in a dedicated Skill Library collection. Procedural memories are immune to activation decay and shared fleet-wide, ensuring that hard-won operational knowledge persists indefinitely and is retrievable by any agent in the mesh.

**How it works:** When the pipeline detects procedural language -- sequential steps ("step 1", "step 2"), how-to patterns ("how to", "to do X"), command sequences, operational runbooks -- it automatically classifies the memory as `procedural` type and routes it to the Skill Library collection instead of the general shared memories collection. Procedural memories have two special properties: (1) immunity to activation decay -- a runbook stored six months ago has the same retrievability as one stored today, and (2) fleet-wide sharing -- skills are accessible to every agent in the mesh, not siloed to the creating agent. The skill library grows naturally as agents encounter and store operational procedures.

**Why it matters:** Operational knowledge is the highest-value category of memory for most agent deployments. A deployment runbook, a debugging procedure, a configuration guide -- these are the memories that prevent incidents, speed up resolution, and encode hard-won expertise. Making them immune to decay and fleet-wide shared ensures that this knowledge is never lost and always available.

**Code example:**

```typescript
// Procedural detection happens automatically
await m.store(
  "To rotate the SSL certificate on my-server: " +
  "1. Generate new cert with certbot " +
  "2. Copy to /etc/nginx/ssl/ " +
  "3. Run nginx -t to validate " +
  "4. Reload nginx with systemctl reload nginx " +
  "5. Verify with curl -vI https://example.com"
);
// Type: procedural -> stored in Skill Library
// Immune to decay -> always retrievable, even after months
// Fleet-wide -> any agent can recall this

// Querying the skill library
const skills = await m.recall("how to rotate SSL certificates");
// Returns the procedure even if stored months ago by a different agent

// CLI access
// $ mnemosyne skills "nginx"
// $ mnemosyne skills "deployment"
// $ mnemosyne skills  (lists all skills)
```

---

### Feature 33: Mesh Sync

**What it does:** Provides named, versioned shared state blocks that all agents in the mesh can read and write. Blocks are stored as core memories with maximum confidence, meaning they participate in retrieval, reasoning, and consolidation alongside organic memories. Think of them as shared whiteboards visible to every agent.

**How it works:** Each block has a name (e.g., "project_status"), content (arbitrary text), version number (auto-incremented on each write), last writer (agent ID), and update timestamp. Blocks are stored as core memories in the shared memories collection with maximum confidence (1.0) and the `core` type (immune to decay). When a block is written or updated, a broadcast event notifies all agents in the mesh. Reading a block is a direct lookup by name. Because blocks are core memories, they also appear in regular `memory_recall` results when semantically relevant -- a query about "project status" will surface the `project_status` block alongside any organic memories about the project.

**Why it matters:** Multi-agent systems need shared state: project status, team preferences, active priorities, configuration values. Without mesh sync, agents must coordinate through conversational protocols ("Hey Agent-B, what's the project status?"). With it, any agent can instantly read the latest shared state and update it for the rest of the mesh. The versioning system provides optimistic concurrency, and the core memory storage ensures blocks are always retrievable and never decay.

**Code example:**

```typescript
// Write a shared block
await m.blockSet("project_status",
  "Project Alpha: Phase 2 in progress. Backend API 80% complete. " +
  "Frontend blocked on design review. Target launch: March 15."
);
// Stored as core memory, max confidence, version 1
// Broadcast to all agents

// Read from any agent in the mesh
const status = await m.blockGet("project_status");
// Returns: { content: "Project Alpha: Phase 2...", version: 1,
//            lastWriter: "agent-pm", updatedAt: "2026-02-23T10:00:00Z" }

// Update with new version
await m.blockSet("project_status",
  "Project Alpha: Phase 2 in progress. Backend API 95% complete. " +
  "Frontend unblocked -- design approved. Target launch: March 15."
);
// Version incremented to 2, broadcast to all agents

// Blocks are also findable via regular recall
const results = await m.recall("what is the project status?");
// Returns the project_status block alongside organic memories about the project
```

---

## AGI-Grade Capabilities

The capabilities below represent the frontier of cognitive AI systems -- techniques that exist almost exclusively in academic literature and closed research labs. Mnemosyne is the first system to implement all ten as production-ready, deployable infrastructure.

### Research-Grade Capability Matrix

| Capability | Industry Status | Mnemosyne Status |
|---|---|---|
| Flash Reasoning (chain-of-thought graph traversal) | Research paper only | **Production-ready** |
| Theory of Mind for agents (ToMA) | Research paper only | **Production-ready** |
| Observational memory compression | Research paper only | **Production-ready** |
| Reinforcement learning on memory | Research paper only | **Production-ready** |
| Autonomous self-improving consolidation | Not implemented anywhere | **Production-ready** |
| Cross-agent shared cognitive state (Mesh Sync) | Not implemented anywhere | **Production-ready** |
| Bi-temporal knowledge graph | Research paper only | **Production-ready** |
| Proactive anticipatory recall | Not implemented anywhere | **Production-ready** |
| Procedural memory / skill library | Not implemented anywhere | **Production-ready** |
| Session survival across context resets | Not implemented anywhere | **Production-ready** |

### What Makes These AGI-Grade

**Flash Reasoning** implements chain-of-thought traversal through linked memory graphs -- a capability described in papers on cognitive architectures (Soar, ACT-R) but never implemented in a production AI memory system. Most memory systems return isolated results; Mnemosyne reconstructs multi-step reasoning chains that reveal causality and consequences.

**Theory of Mind for Agents** enables computational modeling of what other agents know -- a concept from developmental psychology (Baron-Cohen, 1985) and multi-agent systems research (Gmytrasiewicz & Doshi, 2005) that has never been deployed as production infrastructure. Mnemosyne makes it a queryable API: "What does Agent-B know about topic X?"

**Observational Memory Compression** mirrors the selective encoding of human working memory (Baddeley, 2000) -- converting raw conversational streams into structured, high-signal long-term memories. Research systems have proposed this in theory; Mnemosyne does it after every conversation at 10:1 compression ratios.

**Reinforcement Learning on Memory** applies reward signals to individual memory objects -- promoting memories that prove useful and flagging those that mislead. This is a novel application of reinforcement learning principles (Sutton & Barto, 2018) to knowledge management that has no parallel in existing production systems.

**Autonomous Self-Improving Consolidation** implements four-phase memory maintenance (contradiction detection, dedup merge, popular promotion, stale demotion) without human intervention. Sleep-phase memory consolidation is well-studied in neuroscience (Walker & Stickgold, 2004) but has never been implemented as autonomous infrastructure for AI agents.

**Cross-Agent Shared Cognitive State** goes beyond simple key-value shared state by storing blocks as core memories that participate in the full cognitive pipeline: retrieval, reasoning, decay, and consolidation. No existing multi-agent framework provides shared state with cognitive semantics.

**Bi-Temporal Knowledge Graph** tracks both real-world time and knowledge-acquisition time on every graph edge, enabling temporal queries ("What was true as of date X?") that standard knowledge graphs cannot answer. Bi-temporal databases exist (Oracle, IBM DB2) but bi-temporal knowledge graphs for AI memory are purely academic.

**Proactive Anticipatory Recall** predicts what an agent needs before it asks by generating speculative queries from the incoming prompt. Predictive caching and prefetching exist in database systems, but anticipatory memory recall for AI agents has only been proposed in research papers on cognitive architectures.

**Procedural Memory / Skill Library** gives operational procedures first-class treatment with decay immunity and fleet-wide sharing. The concept comes from cognitive psychology's procedural memory model (Tulving, 1985), but no production AI memory system distinguishes procedural knowledge from other types or protects it from decay.

**Session Survival Across Context Resets** captures and restores cognitive state across context window compactions -- a problem unique to LLM-based agents that has no parallel in traditional software. No existing system provides automatic snapshot/recovery that makes context resets invisible to both the agent and the user.

**Mnemosyne is the first system to combine all ten of these capabilities in a single, production-deployed architecture.** Each feature is independently validated and running at scale -- not a roadmap item, not a proof of concept, not a demo. This is the cognitive foundation that autonomous agents have been missing.

---

## Feature Interaction Map

The 33 features do not operate in isolation. They form a deeply interconnected system where each feature amplifies the others:

- **Reinforcement Learning** promotes memories to `core` type, which makes them **immune to Activation Decay** and elevates them in **Multi-Signal Scoring**
- **Active Consolidation** uses **Smart Dedup & Merge** thresholds to find near-duplicates and the **Confidence System** to resolve contradictions
- **Flash Reasoning** traverses links created by **Auto-Linking** and enriched by **Entity Extraction** and **Graph Ingestion**
- **Intent-Aware Retrieval** adjusts weights in **Multi-Signal Scoring** based on the detected intent, which influences which results survive **Diversity Reranking**
- **Cross-Agent Synthesis** depends on **Pub/Sub Broadcast** for awareness of new memories and the **Confidence System** for elevating corroborated facts to Mesh Fact tier
- **Proactive Recall** uses **Session Survival** snapshots for context recovery and the **7-Type Memory Taxonomy** to generate type-appropriate speculative queries
- **Observational Memory** feeds new memories into the **12-Step Pipeline**, which triggers **Auto-Linking**, **Graph Ingestion**, and **Broadcast**
- **Procedural Memory** relies on the **Security Filter** to keep sensitive commands out and the **Skill Library** collection from **Vector Storage** for fleet-wide sharing
- **Mesh Sync** blocks are stored as `core` type in **Vector Storage**, making them immune to **Activation Decay** and findable via **Multi-Signal Scoring** in regular recall

The result is a system where the whole is dramatically greater than the sum of its parts. Each new memory strengthens the graph, refines the scoring model, feeds the consolidation engine, and enriches the knowledge available to every agent in the mesh.

---

## Summary

Mnemosyne's 33 features span the full spectrum from physical storage infrastructure to autonomous self-improvement. The 5-layer architecture ensures clean separation of concerns:

- **Layer 1** provides reliable, fast infrastructure (vector storage, caching, pub/sub, graph, bi-temporal model, soft-delete)
- **Layer 2** transforms raw text into structured knowledge (12-step pipeline, zero-LLM design, security, dedup, extraction, type taxonomy)
- **Layer 3** connects knowledge into a navigable graph (temporal queries, auto-linking, path finding, timeline reconstruction, depth-limited traversal)
- **Layer 4** makes retrieval intelligent and context-aware (activation decay, multi-signal scoring, intent-aware retrieval, diversity reranking, confidence system, priority scoring)
- **Layer 5** enables the system to learn, maintain itself, and collaborate (reinforcement learning, consolidation, flash reasoning, ToMA, cross-agent synthesis, proactive recall, session survival, observational memory, procedural memory, mesh sync)

Every feature described in this document is production-deployed, running at scale across a 10-agent mesh with 13,000+ memories and sub-200ms retrieval latency. The zero-LLM pipeline design ensures that ingestion is fast, deterministic, and free of per-memory API costs. The modular architecture allows each feature to be independently enabled, making adoption gradual and risk-free.

For API-level details on how to invoke each feature, see [api.md](api.md). For deployment instructions, see [deployment.md](deployment.md). For configuration options including feature toggles, see [configuration.md](configuration.md).
